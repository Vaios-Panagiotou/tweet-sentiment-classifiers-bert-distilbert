# ğŸ§  Tweet Sentiment Classifiers: BERT & DistilBERT

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Prerequisites](#prerequisites)
- [Project Structure](#project-structure)
- [Pipeline Walkthrough](#pipeline-walkthrough)
- [Results & Visuals](#results--visuals)
- [Usage](#usage)
- [License](#license)
- [Contact](#contact)

---

## ğŸ“ Overview

This project implements **two tweet sentiment classification pipelines** using HuggingFace Transformers:

âœ… **BERT-base binary classifier**  
âœ… **DistilBERT binary classifier**  

Both models include:

- Preprocessing: emoji conversion, contraction expansion, punctuation normalization  
- Tokenization: using HuggingFace tokenizers  
- Classification: Binary (Positive vs Negative)  
- Handling class imbalance  
- ROC, Confusion Matrix, t-SNE, and visual analysis  

---

## âœ… Prerequisites

Install the required dependencies:

```bash
pip install torch transformers datasets evaluate contractions scikit-learn matplotlib seaborn textblob emoji wordcloud tqdm
```

The notebook also downloads:
- NLTK tokenizer
- Pretrained BERT & DistilBERT weights

---

## ğŸ“‚ Project Structure

```bash
tweet-sentiment-bert/
â”œâ”€â”€ bert_classifier.ipynb          # BERT-base model pipeline
â”œâ”€â”€ distilbert_classifier.ipynb    # DistilBERT model pipeline
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ LICENSE                         # MIT license
```

---

## ğŸ”„ Pipeline Walkthrough

### ğŸ—ï¸ Setup & Imports
- Install essential libraries
- Set global seed for reproducibility

### ğŸ“¥ Data Loading
- Load train / val / test CSV files
- Apply custom tweet preprocessing (emojis, contractions, punctuation, links, etc.)

### âš™ï¸ Tokenization & Datasets
- HuggingFace tokenizer (`bert-base-uncased` or `distilbert-base-uncased`)
- Custom `Dataset` class  
- DataLoader creation with padding

### ğŸ§  Model Architecture

**BERT pipeline:**  
- BERT base encoder  
- Dropout  
- Linear classifier  

**DistilBERT pipeline:**  
- DistilBERT encoder  
- Dropout  
- Linear classifier  

### âš–ï¸ Handling Imbalanced Classes
- Compute class weights with scikit-learn  
- Use weighted CrossEntropy loss

### ğŸš€ Training Loop
- Optimizer: AdamW  
- Scheduler: Cosine with Restarts  
- Evaluation after each epoch  
- Track accuracy, loss, F1-score  
- Save best model

### ğŸ“Š Visual Analysis
- ROC Curve  
- Confusion Matrix  
- Word Clouds (Raw / Clean)  
- t-SNE of CLS embeddings  
- Sentiment polarity distribution  
- Emoji frequency  
- Length distributions  

### ğŸ“ Submission
- Generate `submission.csv`

---

## ğŸ“Š Results & Visuals

### BERT-base

| Metric          | Value  |
|-----------------|--------|
| Accuracy        | ~0.85  |
| Precision       | ~0.85  |
| Recall          | ~0.85  |
| F1â€‘Score        | ~0.85  |

### DistilBERT

| Metric          | Value  |
|-----------------|--------|
| Accuracy        | ~0.85  |
| Precision       | ~0.85  |
| Recall          | ~0.85  |
| F1â€‘Score        | ~0.85  |

### Sample Visuals
âœ… Word clouds (Raw vs Cleaned)  
âœ… Top emoji frequency  
âœ… Sentiment polarity histogram  
âœ… ROC curve (AUC shown)  
âœ… Confusion matrix  
âœ… t-SNE plots of [CLS] embeddings  
âœ… Learning curves  

---

## ğŸš€ Usage

### Clone the repo

```bash
git clone https://github.com/Alphawastaken/tweet-sentiment-classifiers-bert-distilbert.git
cd tweet-sentiment-classifiers-bert-distilbert
```

### Install dependencies

```bash
pip install -r requirements.txt
```

### Run the Notebooks

```bash
jupyter notebook
```

Then open:

âœ… `bert.ipynb`  
âœ… `Distilbert.ipynb`  

And **Run All** cells.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.  
See the [LICENSE](LICENSE) file for full terms.

---

## ğŸ“¬ Contact
  GitHub: [Vaios-Panagiotou]  
